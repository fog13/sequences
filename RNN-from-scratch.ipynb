{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:22:07.470431Z",
     "start_time": "2024-06-13T07:22:06.167467Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "In sequential models, current hidden state is a function of the current input and previous hidden state:\n",
    "\n",
    "\n",
    "### h(t) = f(h(t-1), x(t); W)\n",
    "\n",
    "W are the parameters of function (in our case NN)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a99481283792099"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For RNN:\n",
    "\n",
    "a(t) = W * h(t-1) + U * x(t) + b1\n",
    "h(t) = tanh(a(t))\n",
    "o(t) = V * h(t) + b2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece7e0312ae56bb5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialise device\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Primary device set to GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Primary device set to CPU.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:22:08.918147Z",
     "start_time": "2024-06-13T07:22:08.915662Z"
    }
   },
   "id": "7035227e94be1de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Primary device set to CPU.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "#  Class for a single RNN Cell\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_tensor, prev_hidden_state):\n",
    "        inner_tensor = self.hidden_layer(prev_hidden_state) + self.input_layer(input_tensor)\n",
    "        hidden_tensor = torch.tanh(inner_tensor)\n",
    "        output_tensor = self.output_layer(hidden_tensor)\n",
    "\n",
    "        return output_tensor, hidden_tensor\n",
    "\n",
    "\n",
    "# Class for RNN model composed of one or more RNN cells\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, device='cpu'):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.rnn_cells = nn.ModuleList([RNNCell(input_size, hidden_size, output_size) for _ in range(self.num_layers)])\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "\n",
    "        batch_size = input_sequence.size(0)\n",
    "        sequence_size = input_sequence.size(1)\n",
    "        # Different initialization techniques can be tried. For now, I am sticking to zeros\n",
    "        hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        outputs = torch.zeros(sequence_size, batch_size, self.output_size).to(self.device)\n",
    "\n",
    "        # iterate over each time step\n",
    "        for seq in range(sequence_size):\n",
    "            token_tensor = input_sequence[:, seq, :]\n",
    "            # iterate over each layer in rnn\n",
    "            for i, rnn_cell in enumerate(self.rnn_cells):\n",
    "                y_out, h_out = rnn_cell(token_tensor, hidden_state[i])\n",
    "                token_tensor = y_out\n",
    "\n",
    "                # update hidden state of rnn cells of layer\n",
    "                hidden_state[i] = h_out\n",
    "\n",
    "            outputs[seq] = token_tensor\n",
    "\n",
    "        return outputs.view(outputs.shape[1], outputs.shape[0], outputs.shape[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:22:09.599924Z",
     "start_time": "2024-06-13T07:22:09.592483Z"
    }
   },
   "id": "5fd855c92ec3640d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def sequence_generator(input_df, window_size, stride=1, batch_size=20000):\n",
    "    df_array = input_df.values\n",
    "    start = 0\n",
    "    total_length = len(df_array)\n",
    "    sequences = []\n",
    "    outputs = []\n",
    "    while start < total_length - window_size:\n",
    "        sequences.append(df_array[start : start + window_size])\n",
    "        outputs.append(df_array[start+window_size])\n",
    "        start += stride\n",
    "        \n",
    "        if len(sequences) >= batch_size:\n",
    "            yield np.array(sequences), np.array(outputs)\n",
    "            sequences = []\n",
    "            outputs = []\n",
    "    \n",
    "    if len(sequences) > 0:\n",
    "        yield np.array(sequences), np.array(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:22:10.182905Z",
     "start_time": "2024-06-13T07:22:10.178488Z"
    }
   },
   "id": "575856f552873390",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def write_to_hdf5(input_df, window_size, stride_size, batch_size, storage_path, dataset_name, \n",
    "                  label_name = 'label'):\n",
    "    sequence_data_size = int(np.floor((len(input_df) - window_size) / stride_size ))\n",
    "    num_features = input_df.shape[1]  # Number of features (columns) in the DataFrame\n",
    "    \n",
    "    gen = sequence_generator(input_df, window_size, stride_size, batch_size)\n",
    "    \n",
    "    with h5py.File(storage_path, 'w') as f:\n",
    "        # Create a dataset with pre-allocated memory for sequences and features\n",
    "        dset = f.create_dataset(dataset_name, (sequence_data_size, window_size, num_features), dtype='float32')\n",
    "        y_set = f.create_dataset(label_name, sequence_data_size)\n",
    "        count = 0\n",
    "        \n",
    "        for batch in gen:\n",
    "            features = batch[0]\n",
    "            y = batch[1]\n",
    "            num_data = features.shape[0]\n",
    "            dset[count:count + num_data] = features\n",
    "            y_set[count: count + num_data] = np.squeeze(y)\n",
    "            count += num_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:22:11.068364Z",
     "start_time": "2024-06-13T07:22:11.063500Z"
    }
   },
   "id": "679712c41bee09a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/daily-minimum-temperatures-in-me.csv')\n",
    "\n",
    "window_size = 5\n",
    "stride_size = 1\n",
    "batch_size = 100\n",
    "\n",
    "df['Daily minimum temperatures'] = pd.to_numeric(df['Daily minimum temperatures'], errors='coerce')\n",
    "\n",
    "# Fill any remaining NaNs with a default value, e.g., 0\n",
    "# df['Daily minimum temperatures'].fillna(0, inplace=True)\n",
    "# Calculate the rolling mean with a window of size N\n",
    "# rolling_mean = df['Daily minimum temperatures'].rolling(window=N).mean()\n",
    "\n",
    "# Fill missing values with the rolling mean\n",
    "# df['Daily minimum temperatures'].fillna(value=rolling_mean, inplace=True)\n",
    "\n",
    "\n",
    "df['Daily minimum temperatures'].interpolate(method='linear', inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:23:15.454021Z",
     "start_time": "2024-06-13T07:23:15.421983Z"
    }
   },
   "id": "5171a0e06044690e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:23:16.443884Z",
     "start_time": "2024-06-13T07:23:16.389877Z"
    }
   },
   "cell_type": "code",
   "source": "write_to_hdf5(df[['Daily minimum temperatures']], window_size, stride_size, batch_size, 'meta/sequence.h5', 'sequences')\n",
   "id": "d42809d0f1f59265",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:24:36.007079Z",
     "start_time": "2024-06-13T07:24:36.000936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HDF5Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, dataset_name, label_name = 'label', transform = None):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.label_name = label_name\n",
    "        self.transform = transform\n",
    "        self.file = None\n",
    "        self.data_len = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.data_len is None:\n",
    "            with h5py.File(self.file_path, 'r') as file:\n",
    "                self.data_len = len(file[self.dataset_name])\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.file is None:\n",
    "            self.file = h5py.File(self.file_path, 'r')\n",
    "        x_in = self.file[self.dataset_name][index]\n",
    "        label = self.file[self.label_name][index]\n",
    "        if self.transform is not None:\n",
    "            x_in = self.transform(x_in)\n",
    "        return x_in, label\n",
    "    \n",
    "    def close(self):\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "            self.file = None\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.close()"
   ],
   "id": "9fa2f03681682935",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:24:38.864448Z",
     "start_time": "2024-06-13T07:24:38.861917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hdf_dataset = HDF5Dataset('meta/sequence.h5', 'sequences', 'label')\n",
    "# del hdf_dataset\n",
    "# hdf_dataset.close()"
   ],
   "id": "a198d999aaa81415",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# Open the original HDF5 file\n",
    "file_path = 'meta/sequence.h5'\n",
    "hdf5_file = h5py.File(file_path, 'r')  # Open in read-only mode\n",
    "data = hdf5_file['sequences']\n",
    "labels = hdf5_file['label']\n",
    "\n",
    "# Create new HDF5 files for training and testing data\n",
    "train_file = h5py.File('meta/train_data.h5', 'w')\n",
    "test_file = h5py.File('meta/test_data.h5', 'w')\n",
    "\n",
    "# Create datasets for data in the new files\n",
    "train_dataset = train_file.create_dataset('data', (0,) + data.shape[1:], maxshape=(None,) + data.shape[1:], dtype=data.dtype)\n",
    "test_dataset = test_file.create_dataset('data', (0,) + data.shape[1:], maxshape=(None,) + data.shape[1:], dtype=data.dtype)\n",
    "\n",
    "# Create datasets for labels in the new files\n",
    "train_labels = train_file.create_dataset('label', (0,) + labels.shape[1:], maxshape=(None,) + labels.shape[1:], dtype=labels.dtype)\n",
    "test_labels = test_file.create_dataset('label', (0,) + labels.shape[1:], maxshape=(None,) + labels.shape[1:], dtype=labels.dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:23.156741Z",
     "start_time": "2024-06-13T07:26:23.146376Z"
    }
   },
   "id": "693fe67127e4de27",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:25.850347Z",
     "start_time": "2024-06-13T07:26:25.822485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_segment_size = 90\n",
    "test_segment_size = 10\n",
    "total_samples = data.shape[0]\n",
    "\n",
    "i = 0\n",
    "while i < total_samples:\n",
    "    end_train = min(i + train_segment_size, total_samples)\n",
    "    if end_train > i:  # Check if there is data to process\n",
    "        train_dataset.resize(train_dataset.shape[0] + (end_train - i), axis=0)\n",
    "        train_dataset[-(end_train - i):] = data[i:end_train]\n",
    "        train_labels.resize(train_labels.shape[0] + (end_train - i), axis=0)\n",
    "        train_labels[-(end_train - i):] = labels[i:end_train]\n",
    "\n",
    "    i = end_train\n",
    "    end_test = min(i + test_segment_size, total_samples)\n",
    "    if end_test > i:  # Check if there is data to process\n",
    "        test_dataset.resize(test_dataset.shape[0] + (end_test - i), axis=0)\n",
    "        test_dataset[-(end_test - i):] = data[i:end_test]\n",
    "        test_labels.resize(test_labels.shape[0] + (end_test - i), axis=0)\n",
    "        test_labels[-(end_test - i):] = labels[i:end_test]\n",
    "\n",
    "    i = end_test\n",
    "\n",
    "train_file.close()\n",
    "test_file.close()\n",
    "hdf5_file.close()"
   ],
   "id": "238e9913b6a7816c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:39.340614Z",
     "start_time": "2024-06-13T07:26:39.339010Z"
    }
   },
   "id": "887aadcb49c9be6c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:40.311079Z",
     "start_time": "2024-06-13T07:26:40.307801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = HDF5Dataset('meta/train_data.h5', 'data', 'label')\n",
    "test_dataset = HDF5Dataset('meta/test_data.h5', 'data', 'label')"
   ],
   "id": "26baa41aeff0c757",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:41.497600Z",
     "start_time": "2024-06-13T07:26:41.492174Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.__len__()",
   "id": "da081976647d070",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3285"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:42.222945Z",
     "start_time": "2024-06-13T07:26:42.219677Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.__len__()",
   "id": "c9e351c44e147f76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:27:29.776653Z",
     "start_time": "2024-06-13T07:27:29.774153Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c1a6128d315bcf94",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:42.623576Z",
     "start_time": "2024-06-13T07:26:42.618895Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.__getitem__(0)",
   "id": "c4a927d11e67690b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16.7],\n",
       "        [16.8],\n",
       "        [17.5],\n",
       "        [17.1],\n",
       "        [18.1]], dtype=float32),\n",
       " 16.6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:27:01.468756Z",
     "start_time": "2024-06-13T07:27:01.459380Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.__getitem__(1)",
   "id": "e9e43183f32a687c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[16.8],\n",
       "        [17.5],\n",
       "        [17.1],\n",
       "        [18.1],\n",
       "        [16.6]], dtype=float32),\n",
       " 10.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !jupyter nbconvert --to script RNN-from-scratch.ipynb",
   "id": "97d4ec5b549b626a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
