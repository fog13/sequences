{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:55:54.337314Z",
     "start_time": "2024-06-23T14:55:54.334674Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "import dask.dataframe as dd\n",
    "from HDF5Dataset import HDF5Dataset\n",
    "import dask.array as da\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "from dask_ml.preprocessing import RobustScaler"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "In sequential models, current hidden state is a function of the current input and previous hidden state:\n",
    "\n",
    "\n",
    "### h(t) = f(h(t-1), x(t); W)\n",
    "\n",
    "W are the parameters of function (in our case NN)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a99481283792099"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For RNN:\n",
    "\n",
    "a(t) = W * h(t-1) + U * x(t) + b1\n",
    "h(t) = tanh(a(t))\n",
    "o(t) = V * h(t) + b2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece7e0312ae56bb5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialise device\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Primary device set to GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Primary device set to CPU.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:38:20.975518Z",
     "start_time": "2024-06-23T14:38:20.969344Z"
    }
   },
   "id": "7035227e94be1de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Primary device set to CPU.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "#  Class for a single RNN Cell\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_tensor, prev_hidden_state):\n",
    "        inner_tensor = self.hidden_layer(prev_hidden_state) + self.input_layer(input_tensor)\n",
    "        hidden_tensor = torch.tanh(inner_tensor)\n",
    "        output_tensor = self.output_layer(hidden_tensor)\n",
    "\n",
    "        return output_tensor, hidden_tensor\n",
    "\n",
    "\n",
    "# Class for RNN model composed of one or more RNN cells\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, device='cpu'):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.rnn_cells = nn.ModuleList([RNNCell(input_size, hidden_size, output_size) for _ in range(self.num_layers)])\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "\n",
    "        batch_size = input_sequence.size(0)\n",
    "        sequence_size = input_sequence.size(1)\n",
    "        # Different initialization techniques can be tried. For now, I am sticking to zeros\n",
    "        hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        outputs = torch.zeros(sequence_size, batch_size, self.output_size).to(self.device)\n",
    "\n",
    "        # iterate over each time step\n",
    "        for seq in range(sequence_size):\n",
    "            token_tensor = input_sequence[:, seq, :]\n",
    "            # iterate over each layer in rnn\n",
    "            for i, rnn_cell in enumerate(self.rnn_cells):\n",
    "                y_out, h_out = rnn_cell(token_tensor, hidden_state[i])\n",
    "                token_tensor = y_out\n",
    "\n",
    "                # update hidden state of rnn cells of layer\n",
    "                hidden_state[i] = h_out\n",
    "\n",
    "            outputs[seq] = token_tensor\n",
    "\n",
    "        return outputs.view(outputs.shape[1], outputs.shape[0], outputs.shape[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:38:21.828987Z",
     "start_time": "2024-06-23T14:38:21.819607Z"
    }
   },
   "id": "5fd855c92ec3640d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def pandas_sequence_generator(input_df, window_size, stride=1, batch_size=20000):\n",
    "    df_array = input_df.values\n",
    "    start = 0\n",
    "    total_length = len(df_array)\n",
    "    sequences = []\n",
    "    outputs = []\n",
    "    while start < total_length - window_size:\n",
    "        sequences.append(df_array[start : start + window_size])\n",
    "        outputs.append(df_array[start+window_size])\n",
    "        start += stride\n",
    "        \n",
    "        if len(sequences) >= batch_size:\n",
    "            yield np.array(sequences), np.array(outputs)\n",
    "            sequences = []\n",
    "            outputs = []\n",
    "    \n",
    "    if len(sequences) > 0:\n",
    "        yield np.array(sequences), np.array(outputs)\n",
    "        \n",
    "def dask_sequence_generator(input_df, window_size, stride=1, batch_size=20000):\n",
    "    # Convert Dask DataFrame to Dask Array for easier slicing\n",
    "    df_array = input_df.to_dask_array(lengths=True)\n",
    "\n",
    "    # Compute total length using the .shape attribute of the Dask Array\n",
    "    total_length = df_array.shape[0]\n",
    "    \n",
    "    # Initialize start index and lists for sequences and outputs\n",
    "    start = 0\n",
    "    sequences = []\n",
    "    outputs = []\n",
    "    \n",
    "    while start < total_length - window_size:\n",
    "        end = start + window_size\n",
    "        # Append the slice of the array (all columns in the window)\n",
    "        sequences.append(df_array[start:end].compute())  # Compute necessary for yielding numpy arrays\n",
    "        \n",
    "        # Outputs could be the next row or specific columns depending on the task\n",
    "        outputs.append(df_array[end].compute())  # Compute the next point\n",
    "        \n",
    "        start += stride\n",
    "        \n",
    "        # Yield batch when enough sequences have been collected\n",
    "        if len(sequences) >= batch_size:\n",
    "            yield np.array(sequences), np.array(outputs)\n",
    "            sequences = []\n",
    "            outputs = []\n",
    "    \n",
    "    # Yield any remaining sequences after the loop\n",
    "    if len(sequences) > 0:\n",
    "        yield np.array(sequences), np.array(outputs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:38:22.920862Z",
     "start_time": "2024-06-23T14:38:22.915983Z"
    }
   },
   "id": "575856f552873390",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def write_to_hdf5(input_df, window_size, stride_size, batch_size, storage_path, dataset_name, \n",
    "                  label_name = 'label'):\n",
    "    sequence_data_size = int(np.floor((len(input_df) - window_size) / stride_size ))\n",
    "    num_features = input_df.shape[1]  # Number of features (columns) in the DataFrame\n",
    "    \n",
    "    gen = dask_sequence_generator(input_df, window_size, stride_size, batch_size)\n",
    "    \n",
    "    with h5py.File(storage_path, 'w') as f:\n",
    "        # Create a dataset with pre-allocated memory for sequences and features\n",
    "        dset = f.create_dataset(dataset_name, (sequence_data_size, window_size, num_features), dtype='float32')\n",
    "        y_set = f.create_dataset(label_name, sequence_data_size)\n",
    "        count = 0\n",
    "        \n",
    "        for batch in gen:\n",
    "            features = batch[0]\n",
    "            y = batch[1]\n",
    "            num_data = features.shape[0]\n",
    "            dset[count:count + num_data] = features\n",
    "            y_set[count: count + num_data] = np.squeeze(y)\n",
    "            count += num_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:38:33.831377Z",
     "start_time": "2024-06-23T14:38:33.818161Z"
    }
   },
   "id": "679712c41bee09a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Since there are some corrupt values, we read it as object/string and then convert it to float. \n",
    "# Reading invalid values as float is throwing error in Dask so using this approach to get around the issue\n",
    "df = dd.read_csv('data/daily-minimum-temperatures-in-me.csv', dtype={'Daily minimum temperatures': 'object'})\n",
    "\n",
    "# Convert the column to numeric float16\n",
    "df['Daily minimum temperatures'] = dd.to_numeric(df['Daily minimum temperatures'], errors='coerce')\n",
    "df['Daily minimum temperatures'] = df['Daily minimum temperatures'].astype('float16')\n",
    "\n",
    "# Use map_partitions to apply the pandas interpolate method to each partition\n",
    "df['Daily minimum temperatures'] = df['Daily minimum temperatures'].map_partitions(\n",
    "    lambda s: s.interpolate(method='linear'), meta=('x', 'float16'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:38:38.503663Z",
     "start_time": "2024-06-23T14:38:38.434011Z"
    }
   },
   "id": "5171a0e06044690e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:46:14.562755Z",
     "start_time": "2024-06-23T14:46:14.473432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply scaler and also save the columns and their order\n",
    "df = df.set_index('Date', drop=True)\n",
    "display(df.head())\n",
    "column_order = list(df.columns)\n",
    "print(column_order)\n",
    "with open('meta/model_column_order.json', 'w') as f:\n",
    "    json.dump(column_order, f)\n",
    "    \n",
    "# Create and fit the scaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(df)\n",
    "\n",
    "scaled_df = scaler.transform(df)"
   ],
   "id": "7c2e39e50eda90a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          Daily minimum temperatures\n",
       "Date                                \n",
       "1/1/1981                   20.703125\n",
       "1/1/1982                   17.000000\n",
       "1/1/1983                   18.406250\n",
       "1/1/1984                   19.500000\n",
       "1/1/1985                   13.296875"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Daily minimum temperatures</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/1981</th>\n",
       "      <td>20.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/1982</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/1983</th>\n",
       "      <td>18.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/1984</th>\n",
       "      <td>19.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/1/1985</th>\n",
       "      <td>13.296875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daily minimum temperatures']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:49:30.833886Z",
     "start_time": "2024-06-23T14:49:30.767011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find diff between max and min\n",
    "print(df['Daily minimum temperatures'].min().compute())\n",
    "print((scaled_df['Daily minimum temperatures'].max() - df['Daily minimum temperatures'].min()).compute())"
   ],
   "id": "5703e255b51658e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.682191780821918\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:50:56.178182Z",
     "start_time": "2024-06-23T14:50:00.401250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 5\n",
    "stride_size = 1\n",
    "batch_size = 100\n",
    "write_to_hdf5(scaled_df[['Daily minimum temperatures']], window_size,\n",
    "              stride_size, batch_size, 'meta/sequence.h5', 'sequences')\n",
    "\n",
    "partitioned_df = scaled_df.repartition(npartitions = 5)\n",
    "write_to_hdf5(partitioned_df[['Daily minimum temperatures']], window_size,\n",
    "              stride_size, batch_size, 'meta/partitioned_sequence.h5', 'sequences')\n",
    "\n"
   ],
   "id": "d42809d0f1f59265",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:51:04.079738Z",
     "start_time": "2024-06-23T14:51:04.072551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Check if partitioned dataframe has any affect on file generated\n",
    "\n",
    "file_path = 'meta/sequence.h5'\n",
    "hdf5_file = h5py.File(file_path, 'r')\n",
    "data = hdf5_file['sequences']\n",
    "\n",
    "file_path = 'meta/partitioned_sequence.h5'\n",
    "p_hdf5_file = h5py.File(file_path, 'r')\n",
    "p_data = p_hdf5_file['sequences']\n",
    "\n",
    "print(data.shape)\n",
    "print(p_data.shape)\n",
    "\n",
    "assert len(data) == len(p_data)"
   ],
   "id": "9fa2f03681682935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3645, 5, 1)\n",
      "(3645, 5, 1)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# Open the original HDF5 file\n",
    "file_path = 'meta/sequence.h5'\n",
    "hdf5_file = h5py.File(file_path, 'r')  # Open in read-only mode\n",
    "data = hdf5_file['sequences']\n",
    "labels = hdf5_file['label']\n",
    "\n",
    "# Create new HDF5 files for training and testing data\n",
    "train_file = h5py.File('meta/train_data.h5', 'w')\n",
    "test_file = h5py.File('meta/test_data.h5', 'w')\n",
    "\n",
    "# Create datasets for data in the new files\n",
    "train_dataset = train_file.create_dataset('data', (0,) + data.shape[1:], maxshape=(None,) + data.shape[1:], dtype=data.dtype)\n",
    "test_dataset = test_file.create_dataset('data', (0,) + data.shape[1:], maxshape=(None,) + data.shape[1:], dtype=data.dtype)\n",
    "\n",
    "# Create datasets for labels in the new files\n",
    "train_labels = train_file.create_dataset('label', (0,) + labels.shape[1:], maxshape=(None,) + labels.shape[1:], dtype=labels.dtype)\n",
    "test_labels = test_file.create_dataset('label', (0,) + labels.shape[1:], maxshape=(None,) + labels.shape[1:], dtype=labels.dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T14:51:18.247689Z",
     "start_time": "2024-06-23T14:51:18.236599Z"
    }
   },
   "id": "693fe67127e4de27",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:51:29.761280Z",
     "start_time": "2024-06-23T14:51:29.722962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_segment_size = 90\n",
    "test_segment_size = 10\n",
    "total_samples = data.shape[0]\n",
    "\n",
    "i = 0\n",
    "while i < total_samples:\n",
    "    end_train = min(i + train_segment_size, total_samples)\n",
    "    if end_train > i:  # Check if there is data to process\n",
    "        train_dataset.resize(train_dataset.shape[0] + (end_train - i), axis=0)\n",
    "        train_dataset[-(end_train - i):] = data[i:end_train]\n",
    "        train_labels.resize(train_labels.shape[0] + (end_train - i), axis=0)\n",
    "        train_labels[-(end_train - i):] = labels[i:end_train]\n",
    "\n",
    "    i = end_train\n",
    "    end_test = min(i + test_segment_size, total_samples)\n",
    "    if end_test > i:  # Check if there is data to process\n",
    "        test_dataset.resize(test_dataset.shape[0] + (end_test - i), axis=0)\n",
    "        test_dataset[-(end_test - i):] = data[i:end_test]\n",
    "        test_labels.resize(test_labels.shape[0] + (end_test - i), axis=0)\n",
    "        test_labels[-(end_test - i):] = labels[i:end_test]\n",
    "\n",
    "    i = end_test\n",
    "\n",
    "train_file.close()\n",
    "test_file.close()\n",
    "hdf5_file.close()"
   ],
   "id": "238e9913b6a7816c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:51:40.444666Z",
     "start_time": "2024-06-23T14:51:40.437819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume the paths to your HDF5 files\n",
    "train_file_path = 'meta/train_data.h5'\n",
    "test_file_path = 'meta/test_data.h5'\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = HDF5Dataset(train_file_path, 'data', 'label')\n",
    "test_dataset = HDF5Dataset(test_file_path, 'data', 'label')\n",
    "\n",
    "\n"
   ],
   "id": "e9e43183f32a687c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:56:08.727820Z",
     "start_time": "2024-06-23T14:56:08.718998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save scaler to disk\n",
    "joblib.dump(scaler, 'models/model_robust_scaler.joblib')"
   ],
   "id": "b68c304922f45d89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model_robust_scaler.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T14:57:21.315826Z",
     "start_time": "2024-06-23T14:57:21.304900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, labels in train_loader:\n",
    "        # Your training code here\n",
    "        print(data)\n",
    "        pass"
   ],
   "id": "bfb237099ab03589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T09:08:18.192307Z",
     "start_time": "2024-06-13T09:08:17.067390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!jupyter nbconvert --to script RNN-from-scratch.ipynb\n",
    "\n",
    "h5py.__version__"
   ],
   "id": "97d4ec5b549b626a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook RNN-from-scratch.ipynb to script\r\n",
      "[NbConvertApp] Writing 9994 bytes to RNN-from-scratch.py\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.11.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d09a3beb4062272f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
